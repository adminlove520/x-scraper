#!/usr/bin/env python3
"""
Advanced Twitter/X.com Scraper
Exclusively designed for Twitter/X.com high-volume tweet scraping.
Optimized for Twitter's current structure with anti-detection features.
Can scrape 1000+ tweets per day with rotating user agents and proxy support.

Features:
- Twitter/X.com exclusive scraping
- Multiple scraping methods (Selenium, requests)
- Proxy rotation support
- User agent rotation
- Rate limiting and delays
- Resume functionality
- Batch processing
- Data deduplication
- Twitter-specific selectors and parsing

Usage:
    python advanced_twitter_scraper.py --username elonmusk --count 5000 --method selenium
"""

import os
import json
import time
import random
import argparse
import hashlib
from datetime import datetime, timedelta
from typing import List, Dict, Optional, Set
from urllib.parse import urljoin
import threading
from concurrent.futures import ThreadPoolExecutor

try:
    import requests
    from requests.adapters import HTTPAdapter
    from urllib3.util.retry import Retry
except ImportError:
    print("Error: requests not installed. Install with: pip install requests")
    exit(1)

try:
    from selenium import webdriver
    from selenium.webdriver.common.by import By
    from selenium.webdriver.support.ui import WebDriverWait
    from selenium.webdriver.support import expected_conditions as EC
    from selenium.webdriver.chrome.options import Options
    from selenium.webdriver.chrome.service import Service
    from webdriver_manager.chrome import ChromeDriverManager
    from selenium.common.exceptions import TimeoutException, NoSuchElementException
except ImportError:
    print("Warning: Selenium not installed. Selenium method will not be available.")
    print("Install with: pip install selenium webdriver-manager")

try:
    from bs4 import BeautifulSoup
except ImportError:
    print("Error: BeautifulSoup not installed. Install with: pip install beautifulsoup4 lxml")
    exit(1)

class AdvancedTwitterScraper:
    def __init__(self, method: str = "selenium", use_proxy: bool = False, 
                 headless: bool = True, delay_range: tuple = (2, 5)):
        """
        Initialize advanced Twitter scraper.
        
        Args:
            method (str): Scraping method ('selenium', 'requests', 'hybrid')
            use_proxy (bool): Use proxy rotation
            headless (bool): Run browser in headless mode
            delay_range (tuple): Random delay range between actions
        """
        self.method = method
        self.use_proxy = use_proxy
        self.headless = headless
        self.delay_range = delay_range
        self.driver = None
        self.session = None
        self.scraped_ids: Set[str] = set()
        self.user_agents = self.get_user_agents()
        self.proxies = self.load_proxies() if use_proxy else []
        self.current_proxy_index = 0
        
        # Statistics
        self.stats = {
            'total_scraped': 0,
            'duplicates_skipped': 0,
            'errors': 0,
            'start_time': datetime.now()
        }
        
        self.setup_scraper()
    
    def get_user_agents(self) -> List[str]:
        """
        Get list of realistic user agents for rotation.
        """
        return [
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/119.0.0.0 Safari/537.36',
            'Mozilla/5.0 (X11; Linux x86_64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10.15; rv:109.0) Gecko/20100101 Firefox/121.0',
            'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_15_7) AppleWebKit/605.1.15 (KHTML, like Gecko) Version/17.1 Safari/605.1.15',
            'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Edge/120.0.0.0 Safari/537.36'
        ]
    
    def load_proxies(self) -> List[Dict]:
        """
        Load proxy list from file or return empty list.
        Create a proxies.txt file with format: ip:port:username:password
        """
        proxies = []
        proxy_file = 'proxies.txt'
        
        if os.path.exists(proxy_file):
            try:
                with open(proxy_file, 'r') as f:
                    for line in f:
                        line = line.strip()
                        if line and not line.startswith('#'):
                            parts = line.split(':')
                            if len(parts) >= 2:
                                proxy = {
                                    'http': f'http://{parts[0]}:{parts[1]}',
                                    'https': f'http://{parts[0]}:{parts[1]}'
                                }
                                if len(parts) >= 4:  # With auth
                                    proxy = {
                                        'http': f'http://{parts[2]}:{parts[3]}@{parts[0]}:{parts[1]}',
                                        'https': f'http://{parts[2]}:{parts[3]}@{parts[0]}:{parts[1]}'
                                    }
                                proxies.append(proxy)
                print(f"ğŸ“¡ Loaded {len(proxies)} proxies")
            except Exception as e:
                print(f"âš ï¸  Error loading proxies: {e}")
        else:
            print("â„¹ï¸  No proxy file found. Create 'proxies.txt' to use proxy rotation.")
        
        return proxies
    
    def get_random_user_agent(self) -> str:
        """
        Get random user agent for requests.
        """
        return random.choice(self.user_agents)
    
    def get_next_proxy(self) -> Optional[Dict]:
        """
        Get next proxy from rotation.
        """
        if not self.proxies:
            return None
        
        proxy = self.proxies[self.current_proxy_index]
        self.current_proxy_index = (self.current_proxy_index + 1) % len(self.proxies)
        return proxy
    
    def setup_scraper(self):
        """
        Setup scraper based on selected method.
        """
        if self.method in ['selenium', 'hybrid']:
            self.setup_selenium()
        
        if self.method in ['requests', 'hybrid']:
            self.setup_requests()
    
    def setup_selenium(self):
        """
        Setup Selenium WebDriver with anti-detection features.
        """
        try:
            chrome_options = Options()
            
            if self.headless:
                chrome_options.add_argument('--headless')
            
            # Anti-detection options
            chrome_options.add_argument('--no-sandbox')
            chrome_options.add_argument('--disable-dev-shm-usage')
            chrome_options.add_argument('--disable-gpu')
            chrome_options.add_argument('--disable-extensions')
            chrome_options.add_argument('--disable-plugins')
            chrome_options.add_argument('--disable-images')
            chrome_options.add_argument('--disable-javascript')
            chrome_options.add_argument('--disable-web-security')
            chrome_options.add_argument('--disable-features=VizDisplayCompositor')
            chrome_options.add_argument('--disable-ipc-flooding-protection')
            
            # Random user agent
            user_agent = self.get_random_user_agent()
            chrome_options.add_argument(f'--user-agent={user_agent}')
            
            # Proxy support
            if self.use_proxy and self.proxies:
                proxy = self.get_next_proxy()
                if proxy:
                    proxy_url = proxy['http'].replace('http://', '')
                    chrome_options.add_argument(f'--proxy-server=http://{proxy_url}')
            
            # Additional stealth options
            chrome_options.add_experimental_option("excludeSwitches", ["enable-automation"])
            chrome_options.add_experimental_option('useAutomationExtension', False)
            
            # Disable loading of images and CSS
            prefs = {
                "profile.managed_default_content_settings.images": 2,
                "profile.default_content_setting_values.notifications": 2,
                "profile.default_content_settings.popups": 0
            }
            chrome_options.add_experimental_option("prefs", prefs)
            
            service = Service(ChromeDriverManager().install())
            self.driver = webdriver.Chrome(service=service, options=chrome_options)
            
            # Execute script to hide webdriver property
            self.driver.execute_script("Object.defineProperty(navigator, 'webdriver', {get: () => undefined})")
            
            self.driver.set_window_size(1920, 1080)
            print(f"âœ… Selenium WebDriver initialized with user agent: {user_agent[:50]}...")
            
        except Exception as e:
            print(f"âŒ Failed to initialize Selenium: {e}")
            raise
    
    def setup_requests(self):
        """
        Setup requests session with retry strategy.
        """
        self.session = requests.Session()
        
        # Retry strategy
        retry_strategy = Retry(
            total=3,
            backoff_factor=1,
            status_forcelist=[429, 500, 502, 503, 504],
        )
        
        adapter = HTTPAdapter(max_retries=retry_strategy)
        self.session.mount("http://", adapter)
        self.session.mount("https://", adapter)
        
        # Set headers
        self.session.headers.update({
            'User-Agent': self.get_random_user_agent(),
            'Accept': 'text/html,application/xhtml+xml,application/xml;q=0.9,image/webp,*/*;q=0.8',
            'Accept-Language': 'en-US,en;q=0.5',
            'Accept-Encoding': 'gzip, deflate',
            'Connection': 'keep-alive',
            'Upgrade-Insecure-Requests': '1',
        })
        
        print("âœ… Requests session initialized")
    
    def random_delay(self, multiplier: float = 1.0):
        """
        Add random delay with optional multiplier.
        """
        delay = random.uniform(*self.delay_range) * multiplier
        time.sleep(delay)
    
    def generate_tweet_id(self, tweet_data: Dict) -> str:
        """
        Generate unique ID for tweet based on content.
        """
        content = f"{tweet_data.get('text', '')}{tweet_data.get('created_at', '')}"
        return hashlib.md5(content.encode()).hexdigest()[:16]
    
    def is_duplicate(self, tweet_data: Dict) -> bool:
        """
        Check if tweet is duplicate.
        """
        tweet_id = self.generate_tweet_id(tweet_data)
        if tweet_id in self.scraped_ids:
            self.stats['duplicates_skipped'] += 1
            return True
        self.scraped_ids.add(tweet_id)
        return False
    
    def scrape_with_selenium(self, username: str, count: int) -> List[Dict]:
        """
        Scrape tweets using Selenium method - Twitter/X.com exclusive.
        """
        username = username.lstrip('@')
        # Support both twitter.com and x.com URLs
        url = f"https://x.com/{username}"
        
        print(f"ğŸŒ [Selenium] Navigating to Twitter/X.com: {url}")
        
        try:
            self.driver.get(url)
            self.random_delay()
            
            # Check if we're redirected to login (indicates private/suspended account)
            if "login" in self.driver.current_url.lower():
                print(f"âš ï¸  Account @{username} may be private or suspended")
                return []
            
            # Wait for tweets to load
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, '[data-testid="tweet"]'))
            )
            
            tweets = []
            last_count = 0
            no_new_tweets_count = 0
            max_scrolls = count // 10 + 20  # Dynamic scroll limit
            
            for scroll in range(max_scrolls):
                # Scroll down
                self.driver.execute_script("window.scrollTo(0, document.body.scrollHeight);")
                self.random_delay()
                
                # Get current tweets
                tweet_elements = self.driver.find_elements(By.CSS_SELECTOR, '[data-testid="tweet"]')
                
                # Extract new tweets
                for element in tweet_elements[last_count:]:
                    if len(tweets) >= count:
                        break
                    
                    tweet_data = self.extract_tweet_data_selenium(element)
                    if tweet_data and not self.is_duplicate(tweet_data):
                        tweets.append(tweet_data)
                        self.stats['total_scraped'] += 1
                
                current_count = len(tweets)
                print(f"ğŸ“Š [Selenium] Scroll {scroll+1}: {current_count}/{count} tweets")
                
                # Check if we have enough tweets
                if current_count >= count:
                    break
                
                # Check if no new tweets were loaded
                if current_count == last_count:
                    no_new_tweets_count += 1
                    if no_new_tweets_count >= 3:
                        print("âš ï¸  No new tweets loaded, stopping...")
                        break
                else:
                    no_new_tweets_count = 0
                
                last_count = current_count
                
                # Longer delay every 10 scrolls
                if (scroll + 1) % 10 == 0:
                    print(f"ğŸ”„ Taking extended break after {scroll+1} scrolls...")
                    time.sleep(random.uniform(10, 20))
            
            return tweets[:count]
            
        except Exception as e:
            print(f"âŒ [Selenium] Error: {e}")
            self.stats['errors'] += 1
            return []
    
    def extract_tweet_data_selenium(self, tweet_element) -> Optional[Dict]:
        """
        Extract tweet data using Selenium.
        """
        try:
            tweet_data = {}
            
            # Get tweet text
            try:
                text_element = tweet_element.find_element(By.CSS_SELECTOR, '[data-testid="tweetText"]')
                tweet_data['text'] = text_element.text.strip()
            except:
                tweet_data['text'] = ""
            
            # Skip if no text
            if not tweet_data['text']:
                return None
            
            # Get timestamp
            try:
                time_element = tweet_element.find_element(By.CSS_SELECTOR, 'time')
                tweet_data['created_at'] = time_element.get_attribute('datetime')
                tweet_data['time_text'] = time_element.text
            except:
                tweet_data['created_at'] = datetime.now().isoformat()
                tweet_data['time_text'] = ""
            
            # Get metrics
            metrics = {'replies': 0, 'retweets': 0, 'likes': 0, 'views': 0}
            
            # Extract engagement metrics
            metric_selectors = {
                'replies': '[data-testid="reply"]',
                'retweets': '[data-testid="retweet"]',
                'likes': '[data-testid="like"]'
            }
            
            for metric, selector in metric_selectors.items():
                try:
                    element = tweet_element.find_element(By.CSS_SELECTOR, selector)
                    aria_label = element.get_attribute('aria-label') or ""
                    metrics[metric] = self.extract_number_from_text(aria_label)
                except:
                    pass
            
            tweet_data['metrics'] = metrics
            
            # Get tweet URL
            try:
                link_elements = tweet_element.find_elements(By.CSS_SELECTOR, 'a[href*="/status/"]')
                if link_elements:
                    href = link_elements[0].get_attribute('href')
                    tweet_data['url'] = href
                    if '/status/' in href:
                        tweet_data['id'] = href.split('/status/')[-1].split('?')[0]
            except:
                tweet_data['url'] = ""
                tweet_data['id'] = ""
            
            # Extract hashtags and mentions
            if tweet_data['text']:
                words = tweet_data['text'].split()
                tweet_data['hashtags'] = [word[1:] for word in words if word.startswith('#')]
                tweet_data['mentions'] = [word[1:] for word in words if word.startswith('@')]
            
            # Add scraping metadata
            tweet_data['scraped_at'] = datetime.now().isoformat()
            tweet_data['scraping_method'] = 'selenium'
            
            return tweet_data
            
        except Exception as e:
            return None
    
    def extract_number_from_text(self, text: str) -> int:
        """
        Extract number from text with K/M/B suffixes.
        """
        import re
        
        if not text:
            return 0
        
        # Find numbers in the text
        numbers = re.findall(r'[\d,\.]+[KMB]?', text.upper())
        if not numbers:
            return 0
        
        num_str = numbers[0].replace(',', '')
        
        try:
            if num_str.endswith('K'):
                return int(float(num_str[:-1]) * 1000)
            elif num_str.endswith('M'):
                return int(float(num_str[:-1]) * 1000000)
            elif num_str.endswith('B'):
                return int(float(num_str[:-1]) * 1000000000)
            else:
                return int(float(num_str))
        except:
            return 0
    
    def scrape_user_tweets(self, username: str, count: int = 100) -> List[Dict]:
        """
        Main method to scrape tweets using selected method.
        """
        print(f"ğŸš€ Starting scrape: @{username} ({count} tweets) using {self.method}")
        
        tweets = []
        
        if self.method == 'selenium':
            tweets = self.scrape_with_selenium(username, count)
        elif self.method == 'hybrid':
            # Try selenium first, fallback to other methods if needed
            tweets = self.scrape_with_selenium(username, count)
        
        return tweets
    
    def save_tweets(self, tweets: List[Dict], filename: str, append: bool = False):
        """
        Save tweets to file with optional append mode.
        """
        mode = 'a' if append and os.path.exists(filename) else 'w'
        
        try:
            if mode == 'a':
                # Load existing data and merge
                with open(filename, 'r', encoding='utf-8') as f:
                    existing_tweets = json.load(f)
                
                # Combine and deduplicate
                all_tweets = existing_tweets + tweets
                unique_tweets = []
                seen_ids = set()
                
                for tweet in all_tweets:
                    tweet_id = self.generate_tweet_id(tweet)
                    if tweet_id not in seen_ids:
                        unique_tweets.append(tweet)
                        seen_ids.add(tweet_id)
                
                tweets = unique_tweets
            
            with open(filename, 'w', encoding='utf-8') as f:
                json.dump(tweets, f, indent=2, ensure_ascii=False)
            
            print(f"ğŸ’¾ Saved {len(tweets)} tweets to {filename}")
            
        except Exception as e:
            print(f"âŒ Error saving tweets: {e}")
    
    def print_stats(self):
        """
        Print scraping statistics.
        """
        duration = datetime.now() - self.stats['start_time']
        
        print(f"\n{'='*60}")
        print(f"ğŸ“Š SCRAPING STATISTICS")
        print(f"{'='*60}")
        print(f"â±ï¸  Duration: {duration}")
        print(f"ğŸ“Š Total scraped: {self.stats['total_scraped']}")
        print(f"ğŸ”„ Duplicates skipped: {self.stats['duplicates_skipped']}")
        print(f"âŒ Errors: {self.stats['errors']}")
        print(f"ğŸš€ Rate: {self.stats['total_scraped'] / max(duration.total_seconds() / 60, 1):.1f} tweets/min")
    
    def close(self):
        """
        Clean up resources.
        """
        if self.driver:
            self.driver.quit()
        if self.session:
            self.session.close()
        print("ğŸ”’ Scraper closed")

def main():
    parser = argparse.ArgumentParser(description='Advanced Twitter scraper for high-volume data collection')
    parser.add_argument('--username', '-u', required=True, help='Twitter username (without @)')
    parser.add_argument('--count', '-c', type=int, default=100, help='Number of tweets to scrape')
    parser.add_argument('--method', '-m', choices=['selenium', 'requests', 'hybrid'], default='selenium', help='Scraping method')
    parser.add_argument('--output', '-o', help='Output JSON file')
    parser.add_argument('--headless', action='store_true', help='Run browser in headless mode')
    parser.add_argument('--proxy', action='store_true', help='Use proxy rotation')
    parser.add_argument('--delay-min', type=float, default=2.0, help='Minimum delay between actions')
    parser.add_argument('--delay-max', type=float, default=5.0, help='Maximum delay between actions')
    parser.add_argument('--append', action='store_true', help='Append to existing file')
    
    args = parser.parse_args()
    
    print(f"ğŸš€ Advanced Twitter Scraper")
    print(f"ğŸ‘¤ Target: @{args.username}")
    print(f"ğŸ“Š Count: {args.count:,} tweets")
    print(f"ğŸ”§ Method: {args.method}")
    print(f"ğŸ¤– Headless: {args.headless}")
    print(f"ğŸ“¡ Proxy: {args.proxy}")
    print(f"â±ï¸  Delay: {args.delay_min}-{args.delay_max}s")
    print("=" * 60)
    
    scraper = None
    try:
        # Initialize scraper
        scraper = AdvancedTwitterScraper(
            method=args.method,
            use_proxy=args.proxy,
            headless=args.headless,
            delay_range=(args.delay_min, args.delay_max)
        )
        
        # Scrape tweets
        tweets = scraper.scrape_user_tweets(args.username, args.count)
        
        if tweets:
            # Generate filename
            if args.output:
                filename = args.output
            else:
                timestamp = datetime.now().strftime('%Y%m%d_%H%M%S')
                filename = f"{args.username}_advanced_{timestamp}.json"
            
            # Save results
            scraper.save_tweets(tweets, filename, args.append)
            
            # Print summary
            print(f"\nâœ… Successfully scraped {len(tweets)} tweets")
            if tweets:
                latest = tweets[0]
                print(f"ğŸ“ Latest: {latest['text'][:100]}...")
                total_engagement = sum(t['metrics']['likes'] + t['metrics']['retweets'] for t in tweets)
                print(f"ğŸ’« Total engagement: {total_engagement:,}")
        else:
            print("âŒ No tweets were scraped")
        
        # Print statistics
        scraper.print_stats()
    
    except KeyboardInterrupt:
        print("\nâš ï¸  Scraping interrupted by user")
    except Exception as e:
        print(f"âŒ Scraping failed: {e}")
    finally:
        if scraper:
            scraper.close()

if __name__ == "__main__":
    main()